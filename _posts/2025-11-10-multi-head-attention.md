---
categories: learning
date: '2025-11-10'
layout: single
tags: multi-head-attention attention multi-head AI learning tutorial
title: Multi-Head Attention
---
# AI Learning Journey - 2025-11-10

## ğŸ¯ Today's Focus

**Multi-Head Attention** (Depth 2)

ì§€ë‚œì£¼ì— Attention Mechanismê³¼ Positional Encodingì˜ ê¸°ì´ˆë¥¼ ë°°ì› ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ Transformerì˜ í•µì‹¬ì¸ **Multi-Head Attention**ì„ ê¹Šì´ íƒêµ¬í•©ë‹ˆë‹¤.

---

## ğŸ“š Why This Matters Now

### ğŸ“… ê³¼ê±° í•™ìŠµê³¼ì˜ ì—°ê²°

**ì§€ë‚œì£¼ì— ë°°ìš´ ê²ƒë“¤**:
- **Transformer** (depth 1) - Attention ê¸°ë°˜ ì•„í‚¤í…ì²˜
- **Attention Mechanism** (depth 1) - Self-Attention, Q-K-V êµ¬ì¡°
- **Positional Encoding** (depth 2) - ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©

**ì˜¤ëŠ˜ ë°°ìš¸ ê²ƒ**:
- **Multi-Head Attention** (depth 2) - ë³‘ë ¬ Attention ë©”ì»¤ë‹ˆì¦˜

**ì™œ í•œ ë‹¨ê³„ ë” ê¹Šì´?**
Transformerì˜ í•µì‹¬ì€ ë‹¨ìˆœí•œ Attentionì´ ì•„ë‹ˆë¼ **ì—¬ëŸ¬ ê°œì˜ Attentionì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” Multi-Head êµ¬ì¡°**ì…ë‹ˆë‹¤. ë‹¨ì¼ Attentionì€ í•˜ë‚˜ì˜ ê´€ì ë§Œ ë³´ì§€ë§Œ, Multi-HeadëŠ” ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œì— ì •ë³´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

### ğŸŒ í˜„ì¬ AI ì‚°ì—…ì—ì„œ

Multi-Head Attentionì€ í˜„ì¬ ëª¨ë“  ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(GPT-4, Claude 3.5, Gemini)ì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. GPT-3ëŠ” 96ê°œì˜ í—¤ë“œë¥¼ ì‚¬ìš©í•˜ê³ , GPT-4ëŠ” ìˆ˜ë°± ê°œì˜ í—¤ë“œë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ LLMì´ ë³µì¡í•œ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë¹„ê²°ì…ë‹ˆë‹¤.

### ğŸ”® ì•ìœ¼ë¡œì˜ í•™ìŠµ ì—¬ì •

**ì´ë²ˆ ì£¼ ë‚´** (depth 3):
- **Multi-Head Attention ìµœì í™”** - Flash Attention, Grouped-Query Attention ë“± ìµœì‹  ë³€í˜•

**ì´ë²ˆ ë‹¬** (depth 2-3):
- **GPT Architecture** - Decoder-only Transformerì˜ ì„¸ë¶€ êµ¬ì¡°
- **BERT Architecture** - Encoder-only Transformer ë¹„êµ

**ë¶„ê¸° ëª©í‘œ** (depth 3+):
- **Mechanistic Interpretability** - Multi-Headê°€ ì‹¤ì œë¡œ ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ì§€ í•´ì„
- **Efficient Transformers** - Linear Attention, Sparse Attention ë“±

---

## ğŸ§© Core Concept Explained

### ğŸ¤” ì™œ Single-Head Attentionì´ ì•„ë‹ˆë¼ Multi-Head Attentionì¸ê°€?

**ë¹„ìœ **: ì±…ì„ ì½ì„ ë•Œ í•œ ê°€ì§€ ê´€ì ë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ë²•ì  ê´€ì , ì˜ë¯¸ë¡ ì  ê´€ì , ë§¥ë½ì  ê´€ì ì„ ë™ì‹œì— ë´ì•¼ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆì£ .

Single-Head Attentionì€ í•˜ë‚˜ì˜ Q-K-V ë³€í™˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì–¸ì–´ëŠ” ë‹¤ì¸µì ì…ë‹ˆë‹¤:
- **êµ¬ë¬¸ ê´€ê³„** (ì£¼ì–´-ë™ì‚¬)
- **ì˜ë¯¸ ê´€ê³„** (ë™ì˜ì–´, ë°˜ì˜ì–´)
- **ë¬¸ë§¥ ê´€ê³„** (ëŒ€ëª…ì‚¬ ì°¸ì¡°)

Multi-Head Attentionì€ ì´ ëª¨ë“  ê´€ì ì„ **ë™ì‹œì—, ë…ë¦½ì ìœ¼ë¡œ** í•™ìŠµí•©ë‹ˆë‹¤.

**ìˆ˜ì‹**:
```
Single-Head: Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V

Multi-Head:
  head_i = Attention(Q W_i^Q, K W_i^K, V W_i^V)
  MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O
```

ì—¬ê¸°ì„œ:
- `h`: í—¤ë“œ ê°œìˆ˜ (GPT-3: 96, BERT: 12)
- `W_i^Q, W_i^K, W_i^V`: ê° í—¤ë“œì˜ ë…ë¦½ì ì¸ ë³€í™˜ í–‰ë ¬
- `W^O`: ì¶œë ¥ ë³€í™˜ í–‰ë ¬

### âš™ï¸ Multi-Head Attentionì€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?

**ë‹¨ê³„ë³„ ì‘ë™ ì›ë¦¬**:

1. **ì…ë ¥ ë¶„í• **: d_model ì°¨ì›ì„ hê°œ í—¤ë“œë¡œ ë‚˜ëˆ”
   ```
   d_model = 512, h = 8
   â†’ ê° í—¤ë“œëŠ” 512/8 = 64 ì°¨ì› ì²˜ë¦¬
   ```

2. **ë…ë¦½ì  ë³€í™˜**: ê° í—¤ë“œê°€ ìì‹ ë§Œì˜ Q, K, V í–‰ë ¬ë¡œ ë³€í™˜
   ```python
   for i in range(num_heads):
       Q_i = Q @ W_i_Q  # (seq_len, 64)
       K_i = K @ W_i_K
       V_i = V @ W_i_V
       head_i = attention(Q_i, K_i, V_i)
   ```

3. **ë³‘ë ¬ Attention**: ëª¨ë“  í—¤ë“œê°€ ë™ì‹œì— Attention ê³„ì‚°
   ```
   head_1: ë¬¸ë²•ì  ê´€ê³„ í•™ìŠµ
   head_2: ì˜ë¯¸ì  ìœ ì‚¬ì„± í•™ìŠµ
   head_3: ëŒ€ëª…ì‚¬ ì°¸ì¡° í•™ìŠµ
   ...
   ```

4. **ê²°í•©**: ëª¨ë“  í—¤ë“œ ì¶œë ¥ì„ ì—°ê²°(Concatenate)
   ```python
   output = concat([head_1, head_2, ..., head_h]) @ W_O
   # (seq_len, 64*8) @ (512, 512) = (seq_len, 512)
   ```

**ì½”ë“œ ì˜ˆì‹œ**:
```python
import torch
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        assert d_model % num_heads == 0

        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # 64

        # ê° í—¤ë“œì˜ ë³€í™˜ í–‰ë ¬
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)

    def split_heads(self, x, batch_size):
        """(batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)"""
        x = x.view(batch_size, -1, self.num_heads, self.d_k)
        return x.transpose(1, 2)

    def forward(self, Q, K, V, mask=None):
        batch_size = Q.size(0)

        # 1. Linear transformation
        Q = self.W_Q(Q)  # (batch, seq_len, d_model)
        K = self.W_K(K)
        V = self.W_V(V)

        # 2. Split into multiple heads
        Q = self.split_heads(Q, batch_size)  # (batch, h, seq_len, d_k)
        K = self.split_heads(K, batch_size)
        V = self.split_heads(V, batch_size)

        # 3. Scaled dot-product attention (ë³‘ë ¬ ì‹¤í–‰)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        attention_weights = torch.softmax(scores, dim=-1)
        context = torch.matmul(attention_weights, V)

        # 4. Concatenate heads
        context = context.transpose(1, 2).contiguous()
        context = context.view(batch_size, -1, self.d_model)

        # 5. Final linear transformation
        output = self.W_O(context)

        return output, attention_weights

# Example usage
mha = MultiHeadAttention(d_model=512, num_heads=8)
x = torch.randn(32, 10, 512)  # (batch=32, seq_len=10, d_model=512)
output, weights = mha(x, x, x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")  # (32, 10, 512) - same as input
print(f"Attention weights shape: {weights.shape}")  # (32, 8, 10, 10)
```

**í•µì‹¬**:
- ê° í—¤ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµ
- ë³‘ë ¬ ì²˜ë¦¬ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
- ì—¬ëŸ¬ ê´€ì ì„ í†µí•©í•˜ì—¬ í’ë¶€í•œ í‘œí˜„ ìƒì„±

### â° Multi-Head Attentionì€ ì–¸ì œ ì‚¬ìš©í•˜ëŠ”ê°€?

**ì‚¬ìš© ì‹œì **:
1. **Transformer ê¸°ë°˜ ëª¨ë¸ ì „ì²´**: BERT, GPT, T5, Claude, etc.
2. **ë³µì¡í•œ ë§¥ë½ ì´í•´ í•„ìš” ì‹œ**: ê¸´ ë¬¸ì„œ, ë‹¤ì¸µì  ì˜ë¯¸
3. **ë‹¤ì–‘í•œ ê´€ê³„ í¬ì°© í•„ìš” ì‹œ**: êµ¬ë¬¸+ì˜ë¯¸+ë¬¸ë§¥

**ì‹¤ì œ ì ìš© ì‚¬ë¡€**:
- **GPT-4**: ìˆ˜ë°± ê°œ í—¤ë“œë¡œ ë³µì¡í•œ ì¶”ë¡ 
- **BERT**: 12ê°œ í—¤ë“œë¡œ ì–‘ë°©í–¥ ë§¥ë½ ì´í•´
- **Vision Transformer (ViT)**: ì´ë¯¸ì§€ íŒ¨ì¹˜ ê°„ ê´€ê³„ í•™ìŠµ

**íŠ¸ë ˆì´ë“œì˜¤í”„**:
- ì¥ì : í‘œí˜„ë ¥ ë†’ìŒ, ë‹¤ì–‘í•œ íŒ¨í„´ í•™ìŠµ
- ë‹¨ì : ê³„ì‚°ëŸ‰ ì¦ê°€ (í—¤ë“œ ìˆ˜ì— ë¹„ë¡€)

### ğŸ‘¤ Multi-Head Attentionì„ ëˆ„ê°€ ë§Œë“¤ì—ˆê³ , ì™œ ë§Œë“¤ì—ˆëŠ”ê°€?

**Ashish Vaswani et al.** (Google Brain, 2017)

"Attention is All You Need" ë…¼ë¬¸ì—ì„œ Multi-Head Attentionì„ ì œì•ˆ. ë‹¨ì¼ Attentionì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , **ì•™ìƒë¸” íš¨ê³¼**ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ëŒ€í­ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.

**í•µì‹¬ ì¸ì‚¬ì´íŠ¸**:
> "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions."

ì¦‰, ê° í—¤ë“œëŠ” **ë‹¤ë¥¸ í‘œí˜„ ê³µê°„**ì—ì„œ ë‹¤ë¥¸ ì •ë³´ë¥¼ ë™ì‹œì— í¬ì°©í•©ë‹ˆë‹¤.

### ğŸš€ Multi-Head Attentionì€ ì–´ë””ë¡œ ë°œì „í•˜ê³  ìˆëŠ”ê°€?

**ìµœì‹  ë³€í˜•ë“¤**:

1. **Flash Attention (2022)**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ Attention ê³„ì‚°
   - O(NÂ²) â†’ O(N) ë©”ëª¨ë¦¬ ì‚¬ìš©
   - 2-4ë°° ì†ë„ í–¥ìƒ

2. **Grouped-Query Attention (GQA, 2023)**: LLaMA 2, Mistralì—ì„œ ì‚¬ìš©
   - K, V í—¤ë“œ ìˆ˜ ì¤„ì„ (QëŠ” ìœ ì§€)
   - ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ, ì„±ëŠ¥ ìœ ì§€

3. **Multi-Query Attention (MQA)**: PaLM, Falconì—ì„œ ì‚¬ìš©
   - ëª¨ë“  í—¤ë“œê°€ K, V ê³µìœ 
   - ê·¹ë„ë¡œ ë¹ ë¥¸ ì¶”ë¡ , ì•½ê°„ì˜ ì„±ëŠ¥ í•˜ë½

4. **Sparse Attention**: Longformer, BigBird
   - ëª¨ë“  í† í°ì´ ì•„ë‹Œ ì¼ë¶€ë§Œ Attend
   - ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥ (4096+ í† í°)

**ë¯¸ë˜ ë°©í–¥**:
- **Adaptive Attention**: ì…ë ¥ì— ë”°ë¼ í—¤ë“œ ìˆ˜ ë™ì  ì¡°ì ˆ
- **Learned Sparsity**: ì–´ë–¤ í† í°ì„ ë³¼ì§€ í•™ìŠµ
- **Hybrid Architectures**: Attention + State Space Models (Mamba)

---

## ğŸ”— Connections to What You Already Know

### ê³¼ê±° í•™ìŠµê³¼ì˜ ì—°ê²°

**2025-11-03** - Attention Mechanism (depth 1)
- Single-Head Attentionì˜ Q-K-V êµ¬ì¡° í•™ìŠµ
- ì˜¤ëŠ˜: ì´ê²ƒì„ ì—¬ëŸ¬ ê°œ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ëŠ” Multi-Headë¡œ í™•ì¥

**2025-11-07** - Positional Encoding (depth 2)
- Transformerê°€ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ë°©ë²•
- Multi-Head Attentionì€ ì´ ìœ„ì¹˜ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê´€ê³„ í•™ìŠµ

**ê°œë… ì—°ê²°**:
```
Attention Mechanism (depth 1)
    â†“
Multi-Head Attention (depth 2) â† ì˜¤ëŠ˜
    â†“
Flash Attention (depth 3) â† ë‹¤ìŒ ë‹¨ê³„
```

---

## ğŸ¤ Voices from the Field

### ì—°êµ¬ìë“¤ì˜ ì¸ì‚¬ì´íŠ¸

**Ilya Sutskever** (Safe Superintelligence Inc.)
- Interview: Reid Hoffman 2023 (2023-07)
- Duration: ~1h
- Key insight: "Multi-Head Attentionì€ ì•™ìƒë¸” íš¨ê³¼ë¥¼ ë‚´ì¥í•œ êµ¬ì¡°ì…ë‹ˆë‹¤. ê° í—¤ë“œê°€ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•˜ë©´ì„œë„, í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í†µí•©ë©ë‹ˆë‹¤."

**ì‹œì²­í•˜ê¸°**:
```bash
/link https://www.possible.fm/podcast/openai-ilya-sutskever-greg-brockman/
```
---

**Andrej Karpathy** (Eureka Labs)
- Interview: Dwarkesh Patel 2025 (2025-10)
- Duration: ~2h
- Key insight: "Multi-Head Attentionì„ ì²˜ìŒ ë´¤ì„ ë•Œ 'ì´ê±´ ì²œì¬ì ì´ë‹¤'ë¼ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ê° í—¤ë“œê°€ ì „ë¬¸ê°€ì²˜ëŸ¼ ë‹¤ë¥¸ ì—­í• ì„ ë§¡ì•„ìš”."

**ì‹œì²­í•˜ê¸°**:
```bash
/link https://www.dwarkeshpatel.com/p/andrej-karpathy
```
---

**Dario Amodei** (Anthropic)
- Interview: Lex Fridman #452 (2024-11)
- Duration: 5h22m
- Key insight: "ìŠ¤ì¼€ì¼ë§ì˜ í•µì‹¬ì€ ë‹¨ìˆœíˆ íŒŒë¼ë¯¸í„°ë¥¼ ëŠ˜ë¦¬ëŠ” ê²Œ ì•„ë‹ˆë¼, Multi-Headì²˜ëŸ¼ ë³‘ë ¬ì„±ì„ í™œìš©í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤."

**ì‹œì²­í•˜ê¸°**:
```bash
/link https://www.youtube.com/watch?v=ugvHCXCOmm4
```
---

### ì¶”ì²œ í•™ìŠµ ë¦¬ì†ŒìŠ¤

ë‹¤ìŒ ë¦¬ì†ŒìŠ¤ë“¤ì„ í†µí•´ ë” ê¹Šì´ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

**Andrej Karpathy**: Neural Networks: Zero to Hero - Attention êµ¬í˜„ ê°•ì˜
**3Blue1Brown**: Visualizing Attention
**Yannic Kilcher**: Attention is All You Need ë…¼ë¬¸ ìƒì„¸ ë¶„ì„

---

## ğŸ“– Historical Context

### ì—­ì‚¬ì  íƒ€ì„ë¼ì¸

**2017.06** - Multi-Head Attention ì œì•ˆ (Transformer ë…¼ë¬¸)
- Vaswani et al. (Google Brain)
- Single-Head ëŒ€ë¹„ 2-3 BLEU ì ìˆ˜ í–¥ìƒ
- ë…¼ë¬¸: https://arxiv.org/abs/1706.03762

**2018** - BERT (12 í—¤ë“œ), GPT-1 (12 í—¤ë“œ)
- Multi-Headê°€ í‘œì¤€ì´ ë¨

**2019** - GPT-2 (12-48 í—¤ë“œ, ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)
- í—¤ë“œ ìˆ˜ ì¦ê°€ê°€ ì„±ëŠ¥ í–¥ìƒì— ê¸°ì—¬

**2020.05** - GPT-3 (96 í—¤ë“œ)
- ëŒ€ê·œëª¨ ëª¨ë¸ì—ì„œ Multi-Headì˜ í˜ ì…ì¦

**2022** - Flash Attention ë°œí‘œ
- Multi-Head Attentionì˜ ë©”ëª¨ë¦¬/ì†ë„ ë¬¸ì œ í•´ê²°

**2023** - Grouped-Query Attention (GQA)
- LLaMA 2, Mistral ë“±ì—ì„œ ì‚¬ìš©
- ì¶”ë¡  ì†ë„ 2ë°° í–¥ìƒ

**2024** - GPT-4, Claude 3.5
- ìˆ˜ë°± ê°œ í—¤ë“œë¡œ í™•ì¥ (ì •í™•í•œ ìˆ˜ëŠ” ë¹„ê³µê°œ)
- Flash Attention 2 ì ìš©

---

## ğŸ§ª Your Understanding Check

### ğŸ§­ í•™ìŠµ ì ê²€

ë‹¤ìŒ ì§ˆë¬¸ë“¤ì— ìŠ¤ìŠ¤ë¡œ ë‹µí•´ë³´ì„¸ìš”:

1. Multi-Head Attentionì˜ ì‘ë™ ì›ë¦¬ë¥¼ ìˆ˜ì‹/ì½”ë“œë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?
2. ì–´ë–¤ ìƒí™©ì—ì„œ Multi-Headë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ê°€?
3. Multi-Head Attentionì˜ ì¥ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€?
4. ì‹¤ì œ í”„ë¡œì íŠ¸ì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ”ê°€?

### ğŸ’» ì½”ë“œ ì‹¤ìŠµ

ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”:

```python
import torch
import torch.nn as nn

class SimpleMultiHeadAttention(nn.Module):
    def __init__(self, d_model=512, num_heads=8):
        super().__init__()
        # TODO: ì´ˆê¸°í™” ì½”ë“œ ì‘ì„±
        # Hint:
        # - d_k = d_model // num_heads
        # - W_Q, W_K, W_V, W_O ì„ í˜• ë³€í™˜ í•„ìš”
        pass

    def split_heads(self, x, batch_size):
        """
        TODO: (batch, seq_len, d_model) â†’ (batch, num_heads, seq_len, d_k)

        Hint:
        1. x.view(batch_size, -1, num_heads, d_k)
        2. transpose(1, 2) to move heads to second dimension
        """
        pass

    def forward(self, Q, K, V):
        # TODO: Multi-Head Attention êµ¬í˜„
        # 1. Linear transformation (W_Q, W_K, W_V)
        # 2. Split into heads
        # 3. Scaled dot-product attention
        # 4. Concatenate heads
        # 5. Final linear transformation (W_O)
        pass

# Test
mha = SimpleMultiHeadAttention(d_model=512, num_heads=8)
x = torch.randn(2, 10, 512)  # (batch=2, seq_len=10, d_model=512)
output = mha(x, x, x)

assert output.shape == (2, 10, 512), f"Expected (2, 10, 512), got {output.shape}"
print("âœ… Test passed!")
```

**ê²€ì¦**:
- Output shapeì´ inputê³¼ ê°™ì€ì§€ í™•ì¸ (batch, seq_len, d_model)
- ê° í—¤ë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
- Attention weightsì˜ shapeì´ (batch, num_heads, seq_len, seq_len)ì¸ì§€ í™•ì¸

**ë‹µë³€ì€ ë‹¤ìŒ ì„¸ì…˜ì—ì„œ í™•ì¸í•©ë‹ˆë‹¤.**
- ë‹µë³€ì„ ì ì–´ë‘ê³  ë‹¤ìŒ í•™ìŠµ ì‹œ ë¹„êµí•´ë³´ì„¸ìš”.
- ë§‰íˆëŠ” ì§ˆë¬¸ì´ ìˆë‹¤ë©´, ê·¸ ë¶€ë¶„ì„ ë” ê¹Šì´ í•™ìŠµí•  ì‹ í˜¸ì…ë‹ˆë‹¤.

---

## ğŸš€ Next Steps for You

### ì´ë²ˆ ì£¼ (ë‹¨ê¸°)
- Multi-Head Attention ì½”ë“œ ì§ì ‘ êµ¬í˜„ (30ë¶„)
- Attention weights ì‹œê°í™” (ê° í—¤ë“œê°€ ë¬´ì—‡ì„ ë³´ëŠ”ì§€)
- Andrej Karpathyì˜ Attention ê°•ì˜ ì‹œì²­

### ì´ë²ˆ ë‹¬ (ì¤‘ê¸°)
- Flash Attention ë…¼ë¬¸ ì½ê¸° (ë©”ëª¨ë¦¬ ìµœì í™” ì´í•´)
- GPT vs BERT ì•„í‚¤í…ì²˜ ë¹„êµ (Decoder-only vs Encoder-only)
- Grouped-Query Attention (GQA) êµ¬í˜„

### ë¶„ê¸° ëª©í‘œ (ì¥ê¸°)
- Mechanistic Interpretability: ê° í—¤ë“œê°€ ì‹¤ì œë¡œ ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ì§€ ë¶„ì„
- Sparse Attention ë³€í˜•ë“¤ ë¹„êµ (Longformer, BigBird, etc.)
- Efficient Transformers ë…¼ë¬¸ ì„œë² ì´

---

## ğŸŒ± Growth Indicators

### ğŸ“Š ê°œë…ë³„ ì§„í–‰ë¥ 

**Architecture**: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 40%
- ì™„ë£Œ: 2/5 (Transformer, Positional Encoding ë§ˆìŠ¤í„°)
- ì§„í–‰ ì¤‘: 2 (Attention Mechanism, Multi-Head Attention)

**Training**: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20%
- ì™„ë£Œ: 0/3
- ì§„í–‰ ì¤‘: 1 (RLHF)

**Safety**: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0%
- ì™„ë£Œ: 0/3
- ì§„í–‰ ì¤‘: 0

**Model**: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0%
- ì™„ë£Œ: 0/3
- ì§„í–‰ ì¤‘: 0

### ğŸ“ˆ í•™ìŠµ ì†ë„

**ì´ë²ˆ ì£¼**: 1ê°œ ê°œë… (Multi-Head Attention)
**ì´ë²ˆ ë‹¬**: 4ê°œ ê°œë…
**ì¶”ì„¸**: steady â†’ (ê¾¸ì¤€í•œ í•™ìŠµ ì¤‘)

### ğŸ–ï¸ ë‹¬ì„±í•œ ë§ˆì¼ìŠ¤í†¤

- ğŸ† 5ê°œ ê°œë… í•™ìŠµ (Transformer, Attention, RLHF, Scaling Laws, Positional Encoding)
- ğŸ“š Depth 2 ì§„ì… (Positional Encoding, Multi-Head Attention)
- ğŸ”¥ ì£¼ 1íšŒ í•™ìŠµ ë‹¬ì„± (ëª©í‘œ: ì£¼ 3íšŒ)

### ğŸ¯ ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤

- **10ê°œ ê°œë… í•™ìŠµ** (í˜„ì¬ 5ê°œ â†’ 5ê°œ ë” í•„ìš”)
- **3ê°œ ê°œë… ë§ˆìŠ¤í„° (depth 3+)** (í˜„ì¬ 0ê°œ)
- **ì£¼ 3íšŒ í•™ìŠµ ë‹¬ì„±** (í˜„ì¬ ì£¼ 1íšŒ)

---

**Learning Journey Started**: 2025-11-03
**Current Depth Level**: Intermediate (Depth 2 - ë©”ì»¤ë‹ˆì¦˜ê³¼ ì‘ìš© ì´í•´ ì¤‘)
**Sessions Completed**: 4íšŒ
**Next Milestone**: 10ê°œ ê°œë… í•™ìŠµ (5ê°œ ë” í•„ìš”)

---

_Generated by AI Tutor v1.1 | Powered by Claude Code_
