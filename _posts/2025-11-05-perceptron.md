---
categories: learning
date: '2025-11-05'
layout: single
tags: perceptron AI learning tutorial
title: Perceptron
header:
  og_image: /assets/images/og/perceptron.png
---
# AI Learning Journey - 2025-11-05

## ğŸ¯ Today's Focus
**Perceptron** (Depth 1 - Introduction)

## ğŸ“š Why This Matters Now

### ğŸ“… ê³¼ê±° í•™ìŠµê³¼ì˜ ì—°ê²°

**ì§€ë‚œì£¼ì— ë°°ìš´ ê²ƒë“¤**:
- **Neural Network** (depth 1) - 2025-11-04

ì–´ì œ Neural Networkì˜ ì „ì²´ì ì¸ ê°œë…ì„ ë°°ì› ìŠµë‹ˆë‹¤. ë‡Œì˜ êµ¬ì¡°ë¥¼ ëª¨ë°©í•œ ì‹œìŠ¤í…œ, ì¸µ(layer) êµ¬ì¡°, ê°€ì¤‘ì¹˜(weight)ë¥¼ í†µí•œ í•™ìŠµ ë“±ì„ ì´í•´í–ˆì£ .

**ì˜¤ëŠ˜ ë°°ìš¸ ê²ƒ**:
- **Perceptron** (depth 1)

**ì™œ í•œ ë‹¨ê³„ ë” ê¹Šì´?**

Neural Networkë¥¼ ì´í•´í–ˆë‹¤ë©´, ì´ì œ ê·¸ ê¸°ë³¸ ë‹¨ìœ„ì¸ Perceptronì„ ë°°ìš¸ ì°¨ë¡€ì…ë‹ˆë‹¤. Perceptronì€ Neural Networkì˜ "ì›ì"ì™€ ê°™ìŠµë‹ˆë‹¤. ì´ê²ƒì„ ì´í•´í•˜ì§€ ì•Šê³ ëŠ” ë” ë³µì¡í•œ Deep Learningì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

ë¹„ìœ í•˜ìë©´:
- ì–´ì œ: "ì§‘ì€ ë²½ëŒë¡œ ìŒ“ì•„ ì˜¬ë¦° êµ¬ì¡°ë¬¼ì´ë‹¤"ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤
- ì˜¤ëŠ˜: "ë²½ëŒ í•˜ë‚˜ëŠ” ì–´ë–»ê²Œ ìƒê²¼ê³ , ì–´ë–»ê²Œ ìŒ“ëŠ”ê°€"ë¥¼ ë°°ì›ë‹ˆë‹¤

### ğŸŒ í˜„ì¬ AI ì‚°ì—…ì—ì„œ

Perceptronì€ 1958ë…„ì— ë°œëª…ë˜ì—ˆì§€ë§Œ, ì˜¤ëŠ˜ë‚ ì—ë„ ëª¨ë“  Neural Networkì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì…ë‹ˆë‹¤.

**í˜„ëŒ€ AIì—ì„œ Perceptronì˜ ì—­í• **:
- **GPT-4**: ìˆ˜ì‹­ì–µ ê°œì˜ Perceptron(ë‰´ëŸ°)ì´ ì—°ê²°ëœ êµ¬ì¡°
- **BERT**: Perceptronì´ Transformer layer ì•ˆì—ì„œ ì‘ë™
- **CNN (ì´ë¯¸ì§€ ì¸ì‹)**: Convolutional layerë„ Perceptronì˜ ë³€í˜•

Perceptron í•˜ë‚˜ëŠ” ë‹¨ìˆœí•˜ì§€ë§Œ, ìˆ˜ì‹­ì–µ ê°œê°€ ëª¨ì´ë©´ ChatGPTê°€ ë©ë‹ˆë‹¤.

### ğŸ”® ì•ìœ¼ë¡œì˜ í•™ìŠµ ì—¬ì •

**ì´ë²ˆ ì£¼ ë‚´** (depth 2):
- **Perceptronì˜ ìˆ˜í•™ì  ì›ë¦¬** - ê°€ì¤‘ì¹˜ í•©ì‚°ê³¼ í™œì„±í™” í•¨ìˆ˜
- **XOR ë¬¸ì œ** - Perceptronì´ í’€ì§€ ëª»í•˜ëŠ” ë¬¸ì œ

**ì´ë²ˆ ë‹¬** (depth 2-3):
- **Multi-Layer Perceptron (MLP)** - ì—¬ëŸ¬ ì¸µì„ ìŒ“ìœ¼ë©´ ì–´ë–»ê²Œ ë˜ë‚˜
- **Activation Functions** - Sigmoid, ReLU, Tanh
- **Backpropagation** - Perceptronë“¤ì´ ì–´ë–»ê²Œ í•™ìŠµí•˜ëŠ”ê°€

**ë¶„ê¸° ëª©í‘œ** (depth 3+):
- **Gradient Descent** - ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- **CNN** - Perceptronì„ ì´ë¯¸ì§€ì— íŠ¹í™”ì‹œí‚¨ êµ¬ì¡°
- **Transformer** - Attention ë©”ì»¤ë‹ˆì¦˜ (Perceptronì˜ ë°œì „í˜•)

---

## ğŸ§© Core Concept Explained

### ğŸ¤” ì™œ Perceptronì¸ê°€?

**ì—­ì‚¬ì  ë§¥ë½**:

1950ë…„ëŒ€, ì—°êµ¬ìë“¤ì€ "ê¸°ê³„ê°€ í•™ìŠµí•  ìˆ˜ ìˆì„ê¹Œ?"ë¼ëŠ” ì§ˆë¬¸ì„ í–ˆìŠµë‹ˆë‹¤. ì „í†µì  ì»´í“¨í„°ëŠ” ì‚¬ëŒì´ í”„ë¡œê·¸ë¨í•œ ê·œì¹™ëŒ€ë¡œë§Œ ì‘ë™í–ˆì£ .

Frank Rosenblatt (1958)ì€ ìƒë¬¼í•™ì  ë‰´ëŸ°ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë°©í•œ **Perceptron**ì„ ë°œëª…í–ˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ **ìµœì´ˆë¡œ ë°ì´í„°ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê¸°ê³„**ì˜€ìŠµë‹ˆë‹¤.

**ì™œ ì´ê²Œ í˜ëª…ì´ì—ˆë‚˜?**

- ì´ì „: "ê·œì¹™ì„ ì§ì ‘ í”„ë¡œê·¸ë˜ë°" (If-Then ë¡œì§)
- Perceptron: "ë°ì´í„°ë¥¼ ë³´ì—¬ì£¼ë©´ ê·œì¹™ì„ ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ëƒ„"

ì˜ˆë¥¼ ë“¤ì–´:
- ì´ì „: "ì ì´ (x, y) ì¢Œí‘œì—ì„œ x > 5ì´ë©´ ë¹¨ê°•, ì•„ë‹ˆë©´ íŒŒë‘"
- Perceptron: 100ê°œì˜ ì ì„ ë³´ì—¬ì£¼ë©´, ìë™ìœ¼ë¡œ ê²½ê³„ì„ ì„ ì°¾ì•„ëƒ„

### ğŸ“– Perceptronì€ ì •í™•íˆ ë¬´ì—‡ì¸ê°€?

**ì •ì˜**:

Perceptronì€ **ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ ì¸ê³µ ë‰´ëŸ°**ì…ë‹ˆë‹¤.

**êµ¬ì¡°**:
```
ì…ë ¥ë“¤ (x1, x2, ..., xn)
    â†“
ê°€ì¤‘ì¹˜ ê³±ì…ˆ (w1*x1, w2*x2, ...)
    â†“
í•©ì‚° + í¸í–¥(bias): z = w1*x1 + w2*x2 + ... + b
    â†“
í™œì„±í™” í•¨ìˆ˜ (Step function): y = 1 if z â‰¥ 0 else 0
    â†“
ì¶œë ¥ (0 ë˜ëŠ” 1)
```

**ê° ìš”ì†Œ ì„¤ëª…**:

1. **ì…ë ¥ (Input)**: ë°ì´í„° (ì˜ˆ: í‚¤, ëª¸ë¬´ê²Œ)
2. **ê°€ì¤‘ì¹˜ (Weight)**: ê° ì…ë ¥ì˜ "ì¤‘ìš”ë„" (í•™ìŠµìœ¼ë¡œ ì¡°ì •ë¨)
3. **í¸í–¥ (Bias)**: ê²°ì • ê²½ê³„ë¥¼ ì´ë™ì‹œí‚¤ëŠ” ê°’
4. **í™œì„±í™” í•¨ìˆ˜ (Activation)**: í•©ì‚° ê²°ê³¼ë¥¼ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜
5. **ì¶œë ¥ (Output)**: ìµœì¢… ê²°ì • (ì˜ˆ: í•©ê²©/ë¶ˆí•©ê²©)

**ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„**:

```
z = w1*x1 + w2*x2 + ... + wn*xn + b
y = 1 if z â‰¥ 0 else 0
```

ë˜ëŠ” ë²¡í„°ë¡œ:
```
z = wÂ·x + b  (ë‚´ì )
y = step(z)
```

### âš™ï¸ Perceptronì€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?

**ë‹¨ê³„ë³„ ì´í•´**:

**1. ì´ˆê¸°í™”**:
- ê°€ì¤‘ì¹˜ë¥¼ ëœë¤í•˜ê²Œ ì„¤ì • (ì˜ˆ: w = [0.5, -0.3])
- í¸í–¥ë„ ëœë¤ ì„¤ì • (ì˜ˆ: b = 0.1)

**2. ì˜ˆì¸¡ (Forward Pass)**:
- ì…ë ¥ì„ ë°›ìŒ (ì˜ˆ: x = [2, 3])
- ê°€ì¤‘ í•©ì‚°: z = 0.5*2 + (-0.3)*3 + 0.1 = 1.0 - 0.9 + 0.1 = 0.2
- í™œì„±í™”: y = 1 (z â‰¥ 0ì´ë¯€ë¡œ)

**3. í•™ìŠµ (Update Rule)**:
- ì •ë‹µê³¼ ë¹„êµ (ì˜ˆ: ì •ë‹µì€ 0ì¸ë° 1ì„ ì˜ˆì¸¡ â†’ í‹€ë¦¼)
- ê°€ì¤‘ì¹˜ ì¡°ì •: w_new = w_old + learning_rate * (ì •ë‹µ - ì˜ˆì¸¡) * x
- í¸í–¥ ì¡°ì •: b_new = b_old + learning_rate * (ì •ë‹µ - ì˜ˆì¸¡)

**4. ë°˜ë³µ**:
- ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ 2-3 ë‹¨ê³„ ë°˜ë³µ
- ì ì  ë” ì •í™•í•œ ê²½ê³„ì„ ì„ ì°¾ì•„ê°

**ì‹¤ì œ ì˜ˆì‹œ: AND Gate í•™ìŠµ**

ëª©í‘œ: Perceptronì´ AND ë…¼ë¦¬ ê²Œì´íŠ¸ë¥¼ í•™ìŠµ

| x1 | x2 | y (ì •ë‹µ) |
|----|-------|---------|
| 0  | 0     | 0       |
| 0  | 1     | 0       |
| 1  | 0     | 0       |
| 1  | 1     | 1       |

ì´ˆê¸°: w = [0, 0], b = 0 (ëœë¤)

í•™ìŠµ ê³¼ì • (ë‹¨ìˆœí™”):
- ë°ì´í„° (0, 0) â†’ ì˜ˆì¸¡ 0, ì •ë‹µ 0 â†’ ë§ìŒ, ê°€ì¤‘ì¹˜ ìœ ì§€
- ë°ì´í„° (0, 1) â†’ ì˜ˆì¸¡ 0, ì •ë‹µ 0 â†’ ë§ìŒ
- ë°ì´í„° (1, 0) â†’ ì˜ˆì¸¡ 0, ì •ë‹µ 0 â†’ ë§ìŒ
- ë°ì´í„° (1, 1) â†’ ì˜ˆì¸¡ 0, ì •ë‹µ 1 â†’ í‹€ë¦¼! ê°€ì¤‘ì¹˜ ì¦ê°€

10-20ë²ˆ ë°˜ë³µ í›„:
- w = [0.5, 0.5], b = -0.7
- z = 0.5*x1 + 0.5*x2 - 0.7
- (1, 1)ë§Œ z > 0ì´ ë˜ì–´ y = 1 ì¶œë ¥

**ê¸°í•˜í•™ì  ì§ê´€**:

Perceptronì€ 2D í‰ë©´ì„ **ì§ì„ **ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
```
       y
       |
   o   |   x
   o   |   x
-------+---------> x
   o   | x x
       |
```
- o: Class 0
- x: Class 1
- ì§ì„ : w1*x1 + w2*x2 + b = 0

Perceptron í•™ìŠµ = ì´ ì§ì„ ì˜ ìœ„ì¹˜/ê¸°ìš¸ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •

### â° Perceptronì€ ì–¸ì œ ì‚¬ìš©í•˜ëŠ”ê°€?

**ì‚¬ìš©í•˜ê¸° ì¢‹ì€ ê²½ìš°**:
- ë°ì´í„°ê°€ **ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥**í•  ë•Œ (Linearly Separable)
- ê°„ë‹¨í•œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ (Yes/No, 0/1)
- ë‹¤ë¥¸ ë³µì¡í•œ Neural Networkì˜ ê¸°ë³¸ ë‹¨ìœ„ë¡œ

**ì‹¤ì œ ì‘ìš© (í˜„ëŒ€)**:
- Perceptron ë‹¨ë…ìœ¼ë¡œëŠ” ê±°ì˜ ì‚¬ìš© ì•ˆ í•¨
- í•˜ì§€ë§Œ ëª¨ë“  Neural Networkì˜ ê¸°ë³¸ ë¸”ë¡:
  - **Dense Layer**: ìˆ˜ë°± ê°œì˜ Perceptronì´ ë³‘ë ¬ë¡œ
  - **Logistic Regression**: Perceptron + Sigmoid í™œì„±í™”
  - **SVM**: Perceptronì˜ ìˆ˜í•™ì  í™•ì¥

**ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ê²½ìš°**:
- ë¹„ì„ í˜• ë¬¸ì œ (XOR, ì›í˜• ê²½ê³„ ë“±)
- ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ ê°™ì€ ë³µì¡í•œ ë°ì´í„°
- â†’ ì´ëŸ° ê²½ìš° Multi-Layer Perceptron (MLP) í•„ìš”

### ğŸ‘¤ Perceptronì„ ëˆ„ê°€ ë§Œë“¤ì—ˆê³ , ì™œ ë§Œë“¤ì—ˆëŠ”ê°€?

**Frank Rosenblatt (1928-1971)**

- **ë°°ê²½**: ì‹¬ë¦¬í•™ì, Cornell ëŒ€í•™ êµìˆ˜
- **ë™ê¸°**: "ë‡Œì²˜ëŸ¼ í•™ìŠµí•˜ëŠ” ê¸°ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œ?"

**1958ë…„ ë°œëª…**:
- ë…¼ë¬¸: "The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain"
- ì‹¤ì œ í•˜ë“œì›¨ì–´ë¡œ êµ¬í˜„: Mark I Perceptron (IBM 704 ì»´í“¨í„°)
- ìµœì´ˆì˜ **í•™ìŠµ ê°€ëŠ¥í•œ** ì¸ê³µ ë‰´ëŸ°

**ë‹¹ì‹œì˜ í¥ë¶„**:

ì–¸ë¡ ì€ Perceptronì„ "ìƒê°í•˜ëŠ” ê¸°ê³„"ë¡œ ë³´ë„í–ˆìŠµë‹ˆë‹¤. Rosenblattì€ ë‹¤ìŒê³¼ ê°™ì´ ì˜ˆì¸¡í–ˆì£ :

> "Perceptronì€ ê±·ê³ , ë§í•˜ê³ , ë³´ê³ , ì“°ê³ , ìŠ¤ìŠ¤ë¡œ ë³µì œí•˜ê³ , ìì‹ ì˜ ì¡´ì¬ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤."

**AI Winterì˜ ì‹œì‘ (1969)**:

Marvin Minskyì™€ Seymour Papertì´ "Perceptrons" ì±…ì„ ì¶œíŒ:
- **XOR ë¬¸ì œë¥¼ í’€ ìˆ˜ ì—†ë‹¤**ëŠ” ê²ƒì„ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…
- ë‹¨ì¸µ Perceptronì˜ í•œê³„ë¥¼ ëª…í™•íˆ ì§€ì 
- â†’ ì—°êµ¬ ìê¸ˆ ê¸‰ê°, AI ì•”í‘ê¸° ì‹œì‘

í•˜ì§€ë§Œ ê·¸ë“¤ë„ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤:
> "ë‹¤ì¸µ Perceptronì´ë©´ í•´ê²° ê°€ëŠ¥í•  ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ í•™ìŠµ ë°©ë²•ì„ ëª¨ë¥¸ë‹¤."

**ë¶€í™œ (1986)**:

Geoffrey Hinton, David Rumelhart, Ronald Williamsì´ **Backpropagation** ì•Œê³ ë¦¬ì¦˜ì„ ëŒ€ì¤‘í™”:
- ë‹¤ì¸µ Perceptron (Multi-Layer Perceptron, MLP) í•™ìŠµ ê°€ëŠ¥
- Minskyì˜ ì§€ì ì´ì—ˆë˜ XOR ë¬¸ì œ í•´ê²°
- Perceptronì˜ ì§„ì •í•œ ì ì¬ë ¥ ì‹¤í˜„

**Rosenblattì˜ ë¹„ê·¹**:

1971ë…„, Rosenblattì€ 43ì„¸ì— ë³´íŠ¸ ì‚¬ê³ ë¡œ ì‚¬ë§í•©ë‹ˆë‹¤. ê·¸ëŠ” Perceptronì´ Deep Learning í˜ëª…ì˜ ê¸°ë°˜ì´ ë˜ëŠ” ê²ƒì„ ë³´ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

### ğŸš€ Perceptronì€ ì–´ë””ë¡œ ë°œì „í•˜ê³  ìˆëŠ”ê°€?

**í˜„ëŒ€ì˜ Perceptron**:

1. **Activation Function ì§„í™”**:
   - 1958: Step function (0 ë˜ëŠ” 1)
   - 1980s: Sigmoid (ë¶€ë“œëŸ¬ìš´ ê³¡ì„ )
   - 2010s: ReLU (Rectified Linear Unit) - í˜„ì¬ í‘œì¤€
   - 2020s: GELU, Swish - Transformerì—ì„œ ì‚¬ìš©

2. **Normalization ê¸°ë²•**:
   - Batch Normalization (2015): ê° ì¸µì˜ ì¶œë ¥ì„ ì •ê·œí™”
   - Layer Normalization (2016): Transformerì—ì„œ í•„ìˆ˜
   - â†’ Perceptronë“¤ì´ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ

3. **Attention Mechanism**:
   - Perceptronì˜ ê°€ì¤‘ì¹˜ê°€ **ë™ì ìœ¼ë¡œ ë³€í•¨**
   - Self-Attention: ì…ë ¥ì— ë”°ë¼ ê°€ì¤‘ì¹˜ ì¬ê³„ì‚°
   - â†’ GPT, BERTì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

4. **Sparse Networks**:
   - ëª¨ë“  Perceptronì„ ì—°ê²°í•˜ì§€ ì•ŠìŒ
   - Mixture of Experts (MoE): í•„ìš”í•œ Perceptronë§Œ í™œì„±í™”
   - â†’ GPT-4, Geminiì—ì„œ ì‚¬ìš© (ì¶”ì •)

**ë¯¸ë˜ ë°©í–¥**:

- **Neuromorphic Hardware**: Perceptronì„ í•˜ë“œì›¨ì–´ ì¹©ìœ¼ë¡œ êµ¬í˜„
- **Spiking Neural Networks**: ìƒë¬¼í•™ì  ë‰´ëŸ°ì²˜ëŸ¼ ìŠ¤íŒŒì´í¬ ì‹ í˜¸ ì‚¬ìš©
- **Quantum Perceptron**: ì–‘ì ì»´í“¨íŒ…ê³¼ ê²°í•©

---

## ğŸ”— Connections to What You Already Know

### ì–´ì œ ë°°ìš´ Neural Networkì™€ì˜ ì—°ê²°

**ì–´ì œ (2025-11-04)**: Neural Network ì „ì²´ êµ¬ì¡°
- ì¸µ(Layer) êµ¬ì¡°
- ê°€ì¤‘ì¹˜(Weight)ë¡œ ì—°ê²°
- í•™ìŠµì„ í†µí•´ ê°œì„ 

**ì˜¤ëŠ˜ (2025-11-05)**: Perceptron (Neural Networkì˜ ê¸°ë³¸ ë‹¨ìœ„)
- í•˜ë‚˜ì˜ ë‰´ëŸ°
- ê°€ì¤‘ í•©ì‚° + í™œì„±í™” í•¨ìˆ˜
- í•™ìŠµ ê·œì¹™ (Perceptron Learning Rule)

**ê´€ê³„**:
```
Neural Network = ì—¬ëŸ¬ ì¸µì˜ Perceptronë“¤
Perceptron = Neural Networkì˜ ì›ì ë‹¨ìœ„
```

ë¹„ìœ :
- Neural Network = ì§‘ ì „ì²´
- Layer = ì¸µ(1ì¸µ, 2ì¸µ)
- Perceptron = ë²½ëŒ í•˜ë‚˜

### ìˆ˜í•™ê³¼ì˜ ì—°ê²°

**ì„ í˜•ëŒ€ìˆ˜**:
- Perceptron ì—°ì‚° = ë²¡í„° ë‚´ì  (wÂ·x)
- ë‹¤ì¸µ Perceptron = í–‰ë ¬ ê³±ì…ˆ (W * X)

**ê¸°í•˜í•™**:
- Perceptron = ì´ˆí‰ë©´(hyperplane)
- 2D: ì§ì„  (ax + by + c = 0)
- 3D: í‰ë©´ (ax + by + cz + d = 0)
- nD: n-1ì°¨ì› ì´ˆí‰ë©´

**ë…¼ë¦¬í•™**:
- Perceptronìœ¼ë¡œ AND, OR êµ¬í˜„ ê°€ëŠ¥
- í•˜ì§€ë§Œ XORëŠ” ë¶ˆê°€ (ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€)

### ì¼ìƒê³¼ì˜ ì—°ê²°

**ì˜ì‚¬ê²°ì • ê³¼ì •**:

ë‹¹ì‹ ì´ ì˜í™”ë¥¼ ë³¼ì§€ ë§ì§€ ê²°ì •í•œë‹¤ê³  í•´ë´…ì‹œë‹¤:
```
ì…ë ¥:
- x1: í‰ì  (10ì  ë§Œì )
- x2: ì¹œêµ¬ ì¶”ì²œ ì—¬ë¶€ (0 ë˜ëŠ” 1)
- x3: ìƒì˜ ì‹œê°„ (ë¶„)

ê°€ì¤‘ì¹˜ (ë‹¹ì‹ ì˜ ì„ í˜¸):
- w1 = 0.5 (í‰ì  ì¤‘ìš”)
- w2 = 0.3 (ì¹œêµ¬ ì˜ê²¬ ì¤‘ìš”)
- w3 = -0.2 (ë„ˆë¬´ ê¸´ ì˜í™”ëŠ” ì‹«ìŒ)

ê²°ì •:
z = 0.5*í‰ì  + 0.3*ì¹œêµ¬ì¶”ì²œ - 0.2*ìƒì˜ì‹œê°„ + b
y = 1 (ë´„) if z > 0, else 0 (ì•ˆ ë´„)
```

ì´ê²ƒì´ ë°”ë¡œ Perceptronì˜ ì‘ë™ ë°©ì‹ì…ë‹ˆë‹¤! ìš°ë¦¬ì˜ ë‡Œë„ ë¹„ìŠ·í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤.

---

## ğŸ¤ Voices from the Field

### ì—°êµ¬ìë“¤ì˜ ì¸ì‚¬ì´íŠ¸

**Frank Rosenblatt** (1958ë…„ ì¸í„°ë·°):
> "Perceptronì€ ë‹¨ìˆœí•œ ê¸°ê³„ê°€ ì•„ë‹ˆë‹¤. ì´ê²ƒì€ ê²½í—˜ìœ¼ë¡œë¶€í„° ë°°ìš°ê³ , ì‹¤ìˆ˜ë¥¼ í†µí•´ ì„±ì¥í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤. ë§ˆì¹˜ ì–´ë¦°ì•„ì´ê°€ ë°°ìš°ëŠ” ê²ƒì²˜ëŸ¼."

**Marvin Minsky** (1969ë…„, "Perceptrons" ì±…):
> "ë‹¨ì¸µ Perceptronì€ ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì œë§Œ í’€ ìˆ˜ ìˆë‹¤. XORì¡°ì°¨ ëª» í‘¼ë‹¤. ì´ê²ƒì€ ê·¼ë³¸ì  í•œê³„ë‹¤."

(í•˜ì§€ë§Œ Minskyë„ ë‹¤ì¸µ Perceptronì˜ ê°€ëŠ¥ì„±ì€ ì¸ì •í–ˆìŠµë‹ˆë‹¤.)

**Geoffrey Hinton** (2012ë…„ ì¸í„°ë·°):
> "MinskyëŠ” ì˜³ì•˜ë‹¤. í•˜ì§€ë§Œ ê·¸ê°€ ë³´ì§€ ëª»í•œ ê²ƒì€ Backpropagationì´ë‹¤. ë‹¤ì¸µ Perceptronì€ ê±°ì˜ ëª¨ë“  í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆë‹¤."

**Yann LeCun** (2015ë…„ ê°•ì—°):
> "Perceptronì€ Deep Learningì˜ LEGO ë¸”ë¡ì´ë‹¤. í•˜ë‚˜ëŠ” ë‹¨ìˆœí•˜ì§€ë§Œ, ìˆ˜ë°±ë§Œ ê°œê°€ ëª¨ì´ë©´ ë§ˆë²•ì´ ì¼ì–´ë‚œë‹¤."

### ì¶”ì²œ í•™ìŠµ ë¦¬ì†ŒìŠ¤

**ì…ë¬¸ìë¥¼ ìœ„í•œ ë¦¬ì†ŒìŠ¤**:

1. **3Blue1Brown - "But what is a neural network?"**
   - Perceptronì˜ ì‹œê°ì  ì„¤ëª…
   - ê°€ì¤‘ì¹˜ê°€ ì–´ë–»ê²Œ í•™ìŠµë˜ëŠ”ì§€ ì• ë‹ˆë©”ì´ì…˜ìœ¼ë¡œ
   - YouTube ë¬´ë£Œ

2. **Andrej Karpathy - "Neural Networks: Zero to Hero" (Lecture 2)**
   - Perceptronì„ ì½”ë“œë¡œ ì²˜ìŒë¶€í„° êµ¬í˜„
   - NumPyë¡œ 100ì¤„ ì´ë‚´ êµ¬í˜„
   - YouTube ë¬´ë£Œ

3. **StatQuest - "Perceptron Clearly Explained"**
   - í†µê³„ì  ê´€ì ì—ì„œ ì„¤ëª…
   - 10ë¶„ ìš”ì•½ ì˜ìƒ
   - YouTube ë¬´ë£Œ

**ì‹¤ìŠµ**:

1. **Playground.tensorflow.org**
   - ë¸Œë¼ìš°ì €ì—ì„œ Perceptron ì§ì ‘ ì¡°ì‘
   - ê°€ì¤‘ì¹˜ ë³€í™”ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê´€ì°°
   - ë¬´ë£Œ, ì„¤ì¹˜ ë¶ˆí•„ìš”

2. **Kaggle - Perceptron Tutorial**
   - Pythonìœ¼ë¡œ ì§ì ‘ êµ¬í˜„
   - IRIS ë°ì´í„°ì…‹ ë¶„ë¥˜
   - ë¬´ë£Œ, ì˜¨ë¼ì¸ ì‹¤í–‰

---

## ğŸ“– Historical Context

### ì—­ì‚¬ì  íƒ€ì„ë¼ì¸

**1943** - McCulloch-Pitts Neuron
- Perceptronì˜ ì´ë¡ ì  ê¸°ë°˜
- Binary threshold í•¨ìˆ˜
- í•˜ì§€ë§Œ í•™ìŠµ ë¶ˆê°€

**1949** - Hebb's Rule
- Donald Hebb: "Neurons that fire together, wire together"
- Perceptron í•™ìŠµ ê·œì¹™ì˜ ì˜ê°

**1958** - Perceptron ë°œëª… (Frank Rosenblatt)
- ìµœì´ˆì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì¸ê³µ ë‰´ëŸ°
- Mark I Perceptron í•˜ë“œì›¨ì–´ êµ¬í˜„
- ë‰´ìš• íƒ€ì„ì¦ˆ 1ë©´ ê¸°ì‚¬: "ìƒˆë¡œìš´ í•´êµ° ê¸°ê¸°ê°€ ê±·ê³  ë§í•˜ê³  ë³¼ ìˆ˜ ìˆëŠ” ì»´í“¨í„°ë¥¼ ì˜ˆìƒ"

**1960ë…„ëŒ€** - ì´ˆê¸° ì‘ìš©
- ë¬¸ì ì¸ì‹
- íŒ¨í„´ ë¶„ë¥˜
- ë§ì€ ê¸°ëŒ€ì™€ íˆ¬ì

**1969** - "Perceptrons" ì±… ì¶œíŒ (Minsky & Papert)
- XOR ë¬¸ì œ ì¦ëª…
- ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥ ë¬¸ì œëŠ” ëª» í‘¼ë‹¤
- **AI Winter ì‹œì‘**: ì—°êµ¬ ìê¸ˆ ê¸‰ê°

**1974-1986** - ì•”í‘ê¸°
- Perceptron ì—°êµ¬ ê±°ì˜ ì¤‘ë‹¨
- ì¼ë¶€ ì—°êµ¬ìë§Œ ê³„ì† (Hinton, LeCun ë“±)

**1986** - Backpropagation ëŒ€ì¤‘í™”
- Rumelhart, Hinton, Williams ë…¼ë¬¸
- Multi-Layer Perceptron (MLP) í•™ìŠµ ê°€ëŠ¥
- ë…¼ë¬¸: "Learning representations by back-propagating errors"

**1989** - LeCunì˜ LeNet
- CNN (Convolutional Neural Network)
- Perceptronì˜ ë³€í˜•ìœ¼ë¡œ ì†ê¸€ì”¨ ì¸ì‹
- ìš°í¸ë²ˆí˜¸ ìë™ ì¸ì‹ ì‹œìŠ¤í…œ ìƒìš©í™”

**1998** - LeNet-5
- 7ì¸µ CNN
- MNIST ë°ì´í„°ì…‹ì—ì„œ 99.2% ì •í™•ë„

**2012** - AlexNet (ImageNet ìš°ìŠ¹)
- Perceptronì´ ìˆ˜ë°±ë§Œ ê°œ ì—°ê²°ëœ Deep Neural Network
- GPUë¡œ ë³‘ë ¬ í•™ìŠµ
- Deep Learning í˜ëª… ì‹œì‘

**2017-í˜„ì¬** - Transformer ì‹œëŒ€
- Attention ë©”ì»¤ë‹ˆì¦˜ (Perceptronì˜ ë°œì „í˜•)
- GPT, BERT, Claude ë“± ëª¨ë“  LLM
- Perceptronì´ ìˆ˜ì‹­ì–µ ê°œ ì—°ê²°ëœ êµ¬ì¡°

### ì™œ ì´ ì—­ì‚¬ê°€ ì¤‘ìš”í•œê°€?

**êµí›ˆ 1: ë‹¨ìˆœí•¨ì˜ í˜**
- Perceptronì€ 60ë…„ ì „ ë°œëª…
- ê¸°ë³¸ ì•„ì´ë””ì–´ëŠ” ì—¬ì „íˆ ìœ íš¨
- ëª¨ë“  í˜„ëŒ€ AIì˜ ê¸°ë°˜

**êµí›ˆ 2: í•œê³„ë¥¼ ë„˜ì–´ì„œ**
- Minskyì˜ ë¹„íŒì€ ì˜³ì•˜ìŒ (ë‹¨ì¸µì˜ í•œê³„)
- í•˜ì§€ë§Œ í•´ê²°ì±…ë„ ìˆì—ˆìŒ (ë‹¤ì¸µ + Backpropagation)
- ë¬¸ì œë¥¼ ì¸ì •í•˜ê³  ê·¹ë³µí•˜ëŠ” ê²ƒì´ ê³¼í•™

**êµí›ˆ 3: ìŠ¤ì¼€ì¼ì˜ ë§ˆë²•**
- Perceptron 1ê°œ: ê°„ë‹¨í•œ ì„ í˜• ë¶„ë¥˜
- Perceptron 100ê°œ: ë³µì¡í•œ íŒ¨í„´
- Perceptron 1ì–µ ê°œ: ChatGPT

---

## ğŸ§ª Your Understanding Check

### ì´ì „ ì„¸ì…˜ ì§ˆë¬¸ ë³µìŠµ (2025-11-04)

**Q1**: Neural Networkê°€ í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€?

**âœ… ë‹µë³€**:
ì „í†µì  í”„ë¡œê·¸ë˜ë°ì€ ëª¨ë“  ê·œì¹™ì„ ì‚¬ëŒì´ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. Neural NetworkëŠ” ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ì—¬, ê·œì¹™ì„ ëª…ì‹œí•˜ê¸° ì–´ë ¤ìš´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.

**Q2**: Neural Networkì˜ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ë©´?

**âœ… ë‹µë³€**:
ë‡Œì˜ ë‰´ëŸ° êµ¬ì¡°ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë°©í•˜ì—¬, ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

---

### ì˜¤ëŠ˜ ì§ˆë¬¸ (ë‹¤ìŒ ì„¸ì…˜ì—ì„œ ë‹µë³€)

**Q1: Perceptronì€ ì–´ë–»ê²Œ ê²°ì •ì„ ë‚´ë¦¬ëŠ”ê°€?**
- íŒíŠ¸: ê°€ì¤‘ í•©ì‚° â†’ ?

**Q2: XOR ë¬¸ì œëŠ” ë¬´ì—‡ì´ê³ , ì™œ Perceptronì´ í’€ì§€ ëª»í•˜ëŠ”ê°€?**
- íŒíŠ¸: ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥ì„±

**Q3: Perceptron í•™ìŠµ ê·œì¹™ì€ ë¬´ì—‡ì¸ê°€?**
- íŒíŠ¸: w_new = w_old + ?

**Q4: ì‹¤ìƒí™œì—ì„œ Perceptronê³¼ ë¹„ìŠ·í•œ ì˜ì‚¬ê²°ì • ì˜ˆì‹œëŠ”?**
- íŒíŠ¸: ì—¬ëŸ¬ ìš”ì†Œë¥¼ ê°€ì¤‘ í‰ê°€í•˜ì—¬ Yes/No ê²°ì •

### ğŸ’» ì½”ë“œ ì‹¤ìŠµ (ì„ íƒ)

ê°„ë‹¨í•œ Perceptron êµ¬í˜„ (Python):
```python
import numpy as np

class Perceptron:
    def __init__(self, input_size, learning_rate=0.1):
        # TODO: ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™”
        self.weights = np.random.randn(input_size)
        self.bias = np.random.randn()
        self.learning_rate = learning_rate

    def predict(self, x):
        # TODO: ê°€ì¤‘ í•©ì‚° + Step function
        z = np.dot(self.weights, x) + self.bias
        return 1 if z >= 0 else 0

    def train(self, X, y, epochs=100):
        # TODO: Perceptron í•™ìŠµ ê·œì¹™ êµ¬í˜„
        for epoch in range(epochs):
            for xi, yi in zip(X, y):
                prediction = self.predict(xi)
                error = yi - prediction
                # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                self.weights += self.learning_rate * error * xi
                self.bias += self.learning_rate * error

        return self

# í…ŒìŠ¤íŠ¸: AND ê²Œì´íŠ¸ í•™ìŠµ
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])

perceptron = Perceptron(input_size=2)
perceptron.train(X, y, epochs=10)

# ì˜ˆì¸¡
for xi, yi in zip(X, y):
    pred = perceptron.predict(xi)
    print(f"Input: {xi} â†’ Prediction: {pred}, Actual: {yi}")
```

**ë„ì „ ê³¼ì œ**:
1. ìœ„ ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ì„¸ìš”
2. OR ê²Œì´íŠ¸ë„ í•™ìŠµì‹œì¼œë³´ì„¸ìš” (y = [0, 1, 1, 1])
3. XOR ê²Œì´íŠ¸ë¥¼ ì‹œë„í•´ë³´ì„¸ìš” (y = [0, 1, 1, 0]) - ì‹¤íŒ¨í•  ê²ƒì…ë‹ˆë‹¤! ì™œì¼ê¹Œìš”?

**ë‹µë³€ì€ ë‹¤ìŒ ì„¸ì…˜ì—ì„œ í™•ì¸í•©ë‹ˆë‹¤.**
- ì½”ë“œë¥¼ ì‹¤í–‰í•´ë³´ê³  ê²°ê³¼ë¥¼ ê´€ì°°í•˜ì„¸ìš”
- XORê°€ ì‹¤íŒ¨í•˜ëŠ” ì´ìœ ë¥¼ ê¸°í•˜í•™ì ìœ¼ë¡œ ìƒê°í•´ë³´ì„¸ìš”

---

## ğŸš€ Next Steps for You

### Based on Your Current Level: ì…ë¬¸ (Depth 1)

### ì´ë²ˆ ì£¼ (Short-term)

**1. Perceptron ì‹œê°í™” ì´í•´ (30ë¶„)**
   - 3Blue1Brown "Neural Networks" 1-2ì¥ ì‹œì²­
   - Perceptronì´ ì§ì„ ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê³¼ì • ê´€ì°°

**2. ì‹¤ìŠµ: Playground.tensorflow.org (30ë¶„)**
   - 2D ë°ì´í„° ë¶„ë¥˜ ì‹¤í—˜
   - ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥í•œ ë°ì´í„° vs. ë¶ˆê°€ëŠ¥í•œ ë°ì´í„° ë¹„êµ
   - ê°€ì¤‘ì¹˜ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ê´€ì°°

**3. ì½”ë“œ ì‹¤ìŠµ (1ì‹œê°„)**
   - ìœ„ì˜ Perceptron ì½”ë“œ ì‹¤í–‰
   - AND, OR ê²Œì´íŠ¸ í•™ìŠµ
   - XOR ì‹¤íŒ¨ ê²½í—˜ (ì¤‘ìš”!)

### ì´ë²ˆ ë‹¬ (Mid-term)

**1. Perceptron ê¹Šì´ ì´í•´ (Depth 2)**
   - XOR ë¬¸ì œ ê¸°í•˜í•™ì  ì´í•´
   - ì„ í˜• ë¶„ë¦¬ ë¶ˆê°€ëŠ¥ì„± ì¦ëª…
   - Multi-Layer Perceptron í•„ìš”ì„±

**2. Activation Function í•™ìŠµ (Depth 2)**
   - Step function vs. Sigmoid
   - ì™œ ë¶€ë“œëŸ¬ìš´ í™œì„±í™” í•¨ìˆ˜ê°€ í•„ìš”í•œê°€
   - ReLUì˜ ì¥ì 

**3. ê°„ë‹¨í•œ Multi-Layer Perceptron êµ¬í˜„**
   - 2ì¸µ Perceptronìœ¼ë¡œ XOR í•´ê²°
   - Backpropagation ì•Œê³ ë¦¬ì¦˜ ì´í•´
   - NumPyë¡œ ì§ì ‘ êµ¬í˜„

### ë¶„ê¸° ëª©í‘œ (Long-term)

**1. Backpropagation ë§ˆìŠ¤í„° (Depth 3)**
   - Chain Rule ì´í•´
   - Gradient ê³„ì‚° ì†ìœ¼ë¡œ í•´ë³´ê¸°
   - ì™œ Multi-Layerê°€ ì‘ë™í•˜ëŠ”ì§€ ìˆ˜í•™ì  ì´í•´

**2. ì‹¤ì œ ë°ì´í„° ë¶„ë¥˜ í”„ë¡œì íŠ¸**
   - IRIS ë°ì´í„°ì…‹ ë¶„ë¥˜
   - MNIST ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹
   - Kaggle ëŒ€íšŒ ì°¸ê°€

**3. CNN ê¸°ì´ˆ (Depth 3)**
   - Convolutional Layer = Perceptronì˜ íŠ¹ìˆ˜ í˜•íƒœ
   - ì´ë¯¸ì§€ ì¸ì‹ í”„ë¡œì íŠ¸
   - LeNet, AlexNet êµ¬ì¡° ì´í•´

---

## ğŸŒ± Growth Indicators

### ğŸ“Š ê°œë…ë³„ ì§„í–‰ë¥ 

**Architecture**: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20%
- ì™„ë£Œ: 0/10 | ì§„í–‰ ì¤‘: 2 (neural_network depth 1, perceptron depth 1)

**Foundations**: â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 20%
- Neural Network: depth 1 âœ…
- Perceptron: depth 1 âœ… (ì˜¤ëŠ˜)
- Activation Functions: ì˜ˆì •
- Backpropagation: ì˜ˆì •

**Optimization**: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0%
- ì™„ë£Œ: 0/5 | ì§„í–‰ ì¤‘: 0

**Applications**: â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0%
- ì™„ë£Œ: 0/8 | ì§„í–‰ ì¤‘: 0

### ğŸ“ˆ í•™ìŠµ ì†ë„

**ì´ë²ˆ ì£¼**: 2ê°œ ê°œë… (neural_network, perceptron)
**ì´ë²ˆ ë‹¬**: 2ê°œ ê°œë…
**ì¶”ì„¸**: ê°€ì† ì¤‘ â†—ï¸

### ğŸ¯ ë‹¤ìŒ ë§ˆì¼ìŠ¤í†¤

**10ê°œë… ë‹¬ì„±**: 2/10 (20%)
- âœ… neural_network (depth 1)
- âœ… perceptron (depth 1)
- ë‹¤ìŒ: activation_functions (depth 1)

**ì²« Mastery**: 0/1 (depth 3 ë„ë‹¬)
- ëª©í‘œ: perceptron depth 3 ë‹¬ì„± (Backpropagation ì´í•´)
- ì˜ˆìƒ: 2ì£¼ í›„

**ì›”ê°„ ëª©í‘œ**: Multi-Layer Perceptron êµ¬í˜„
- ëª©í‘œ: XOR ë¬¸ì œ í•´ê²°
- ì˜ˆìƒ: ì´ë²ˆ ë‹¬ ë§

---

**Learning Journey Started**: 2025-11-04
**Current Depth Level**: ì…ë¬¸ ë‹¨ê³„ (Depth 1 ì§„í–‰ ì¤‘)
**Sessions Completed**: 2íšŒ
**Next Milestone**: XOR ë¬¸ì œì™€ Multi-Layer Perceptron (ë‹¤ìŒ ì£¼)

---

_Generated by AI Tutor v1.0 | Powered by Claude Code_

## ğŸ“ How to Use This Document

1. **ì²˜ìŒ ì½ì„ ë•Œ**: ì „ì²´ë¥¼ ì­‰ ì½ì–´ë³´ì„¸ìš” (20-25ë¶„)
2. **ì–´ì œì™€ ë¹„êµ**: Neural Networkì™€ Perceptronì˜ ê´€ê³„ë¥¼ ì •ë¦¬í•˜ì„¸ìš”
3. **ì‹¤ìŠµ**: Perceptron ì½”ë“œë¥¼ ê¼­ ì‹¤í–‰í•´ë³´ì„¸ìš” (1ì‹œê°„)
4. **XOR ë„ì „**: XOR ë¬¸ì œë¥¼ ì‹œë„í•˜ê³  ì‹¤íŒ¨ë¥¼ ê²½í—˜í•˜ì„¸ìš” (ì¤‘ìš”!)

**ë‹¤ìŒ ì„¸ì…˜ ì¤€ë¹„**:
- XORê°€ ì™œ ì‹¤íŒ¨í–ˆëŠ”ì§€ ìƒê°í•´ë³´ê¸°
- 2ê°œ ì¸µì´ë©´ ì–´ë–»ê²Œ í•´ê²°ë ì§€ ìƒìƒí•´ë³´ê¸°
- "ì„ í˜• ë¶„ë¦¬ ê°€ëŠ¥"ì´ë¼ëŠ” ë§ì˜ ì˜ë¯¸ ê³ ë¯¼

**í•™ìŠµ ì†ë„ ì¡°ì ˆ**:
- ë„ˆë¬´ ë¹ ë¥´ë‹¤ë©´: Perceptron ì½”ë“œë¥¼ ì§ì ‘ êµ¬í˜„ (NumPy ì—†ì´)
- ë„ˆë¬´ ëŠë¦¬ë‹¤ë©´: Playground.tensorflow.orgë¡œ ì‹œê°ì ìœ¼ë¡œ ì´í•´
- ë”± ë§ë‹¤ë©´: ë‹¤ìŒ ì„¸ì…˜ì—ì„œ Multi-Layer Perceptronìœ¼ë¡œ ì§„í–‰
